{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5318b0e-fa46-40a3-b269-3f3de787c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/JustAnotherArchivist/snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da48ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter\n",
    "import gc\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659514f1-0ba8-4c8c-95df-777841c453f9",
   "metadata": {},
   "source": [
    "### 1) Ticker tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529db0b1-f591-4ebc-a4e4-076afef6d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in year: 2022-03-30 14:45:59+00:00\n",
      "First date in year: 2022-03-04 09:14:13+00:00\n",
      "First date in year: 2022-01-24 21:38:34+00:00\n",
      "First date in year: 2021-12-17 14:53:56+00:00\n",
      "First date in year: 2021-11-10 21:40:56+00:00\n",
      "First date in year: 2021-10-07 17:22:10+00:00\n",
      "First date in year: 2021-08-31 14:06:57+00:00\n",
      "First date in year: 2021-07-20 15:25:45+00:00\n",
      "First date in year: 2021-06-11 19:11:21+00:00\n",
      "First date in year: 2021-05-14 14:09:57+00:00\n",
      "First date in year: 2021-03-24 02:38:09+00:00\n",
      "First date in year: 2021-02-11 18:12:27+00:00\n",
      "First date in year: 2021-01-30 00:26:48+00:00\n",
      "First date in year: 2021-01-28 15:40:44+00:00\n",
      "First date in year: 2021-01-27 10:24:57+00:00\n",
      "First date in year: 2020-04-16 20:05:18+00:00\n",
      "First date in year: 2018-07-23 01:00:21+00:00\n",
      "First date in year: 2015-08-18 12:03:32+00:00\n"
     ]
    }
   ],
   "source": [
    "since = '2012-01-01'\n",
    "until = '2022-05-01'\n",
    "query = '(gme) min_faves:10 since:{0} until:{1}'.format(since,until)\n",
    "#query = '(aapl OR msft OR googl OR amzn OR fb OR tsla OR brk.a OR tsm OR nvda OR jpm) min_faves:10 since:{0} until:{1}'.format(since,until)\n",
    "scraper = snscrape.modules.twitter.TwitterSearchScraper(query)\n",
    "i=0\n",
    "tweets_update = pd.DataFrame()\n",
    "gen = scraper.get_items()\n",
    "while True:\n",
    "    i+=1\n",
    "    try: \n",
    "        tweet = next(gen)\n",
    "        tweets_update = pd.concat([tweets_update, pd.DataFrame([{\"url\": tweet.url,\n",
    "                                        \"date\": tweet.date,\n",
    "                                        \"content\": tweet.content,\n",
    "                                        \"id\": tweet.id,\n",
    "                                        \"username\": tweet.user.username,\n",
    "                                        \"replyCount\": tweet.replyCount,\n",
    "                                        \"retweetCount\": tweet.retweetCount,\n",
    "                                        \"likeCount\": tweet.likeCount,\n",
    "                                        \"lang\": tweet.lang,\n",
    "                                        \"hashtags\": tweet.hashtags,\n",
    "                                        \"cashtags\": tweet.cashtags}])])\n",
    "        \n",
    "        \n",
    "    except StopIteration:\n",
    "        tweets = pd.read_csv(\"data/tweets_tickers.csv\")\n",
    "        tweets = pd.concat([tweets, tweets_update])\n",
    "        tweets.to_csv(\"data/tweets_tickers.csv\",index=False)\n",
    "        del tweets\n",
    "        gc.collect()\n",
    "        break\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print('First date in year: ' + str(pd.to_datetime(tweets_update.date,utc=True).min()))\n",
    "        tweets = pd.read_csv(\"data/tweets_tickers.csv\")\n",
    "        tweets = pd.concat([tweets, tweets_update])\n",
    "        tweets.to_csv(\"data/tweets_tickers.csv\",index=False)\n",
    "        del tweets\n",
    "        gc.collect()\n",
    "        tweets_update = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c79979-c9e9-4820-98c4-6264108e07bd",
   "metadata": {},
   "source": [
    "### 2) Keyword tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9890ddc-62cd-4a40-ad65-e1101853fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    \"AAPL\" : \"(aapl OR apple OR \\\"steve jobs\\\" OR \\\"steve wozniak\\\" OR \\\"ronald wayne\\\" OR \\\"tim cook\\\" OR macintosh OR mac OR ios OR ipod OR iphone OR ipad OR airpods OR homepod OR icloud OR itunes)\",\n",
    "    \"MSFT\" : \"(msft OR microsoft OR \\\"bill gates\\\" OR \\\"paul allen\\\" OR windows OR office OR skype OR \\\"visual studio\\\" OR xbox OR azure OR bing OR linkedin OR yammer OR onedrive OR outlook OR github OR \\\"game pass\\\" OR sharepoint OR \\\"visual studio\\\" OR azure)\",\n",
    "    \"GOOGL\" : \"(googl OR alphabet OR google OR \\\"larry page\\\" OR \\\"sergey brin\\\" OR \\\"sundar pichai\\\" OR gmail OR firebase OR tensorflow OR android OR chrome OR adwords OR nexus OR pixel OR youtube OR deepmind)\",\n",
    "    \"AMZN\" : \"(amzn OR amazon OR \\\"jeff bezos\\\" OR \\\"andy jassy\\\" OR kindle OR alexa OR \\\"fire tv\\\" OR \\\"fire tablet\\\" OR aws OR audible OR goodreads OR imdb OR twitch)\",\n",
    "    \"FB\" : \"(fb OR meta OR \\\"mark zuckerberg\\\" OR zuckerberg OR \\\"eduardo saverin\\\" OR metaverse OR facebook OR messenger OR instagram OR whatsapp OR oculus OR mapillary)\",\n",
    "    \"TSLA\" : \"(tsla OR tesla OR \\\"elon musk\\\" OR elon OR musk OR \\\"model s\\\" OR cybertruck OR \\\"model x\\\" OR \\\"model y\\\" OR \\\"model 3\\\" OR powerwall OR powerpack OR deepscale OR solarcity)\",\n",
    "    \"BRK-A\" : \"(brk.a OR brk.b OR brk OR \\\"berkshire hathaway\\\" OR \\\"oliver chace\\\" OR \\\"warren buffett\\\" OR \\\"buffett\\\" OR \\\"altalink\\\" OR \\\"kern river pipeline\\\" OR \\\"northern natural gas\\\" OR \\\"fruit of the loom\\\" OR \\\"netjets\\\" OR \\\"russell brands\\\")\",\n",
    "    \"TSM\" : \"(2330 OR tsm OR tsmc OR \\\"taiwan semiconductor\\\" OR \\\"morris chang\\\" OR \\\"mark liu\\\" OR cybershuttle OR wafertech OR ssmc)\",\n",
    "    \"NVDA\" : \"(nvda OR nvidia OR \\\"jensen huang\\\" OR geforce OR mellanox OR physx OR deepmap)\",\n",
    "    \"JPM\" : \"(jpm OR jpmorgan OR \\\"j.p. morgan\\\" OR \\\"john pierpont morgan\\\" OR \\\"jamie dimon\\\" OR \\\"chase bank\\\" OR \\\"one equity partners\\\")\",\n",
    "    \"GME\" : \"(gme OR gamestop OR \\\"ryan cohen\\\" OR \\\"matt furlong\\\")\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cea0c8-edfe-438d-959c-8c6856710d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2020-09-06 04:33:24+00:00\n",
      "GOOGL --- First date in year: 2020-08-31 17:26:31+00:00\n",
      "GOOGL --- First date in year: 2020-08-27 01:00:07+00:00\n",
      "GOOGL --- First date in year: 2020-08-22 06:23:14+00:00\n",
      "GOOGL --- First date in year: 2020-08-17 21:57:57+00:00\n",
      "GOOGL --- First date in year: 2020-08-12 11:19:52+00:00\n",
      "GOOGL --- First date in year: 2020-08-06 21:08:59+00:00\n",
      "GOOGL --- First date in year: 2020-08-01 08:34:25+00:00\n",
      "GOOGL --- First date in year: 2020-07-27 02:30:21+00:00\n",
      "GOOGL --- First date in year: 2020-07-21 16:37:51+00:00\n",
      "GOOGL --- First date in year: 2020-07-15 23:44:32+00:00\n",
      "GOOGL --- First date in year: 2020-07-10 09:53:29+00:00\n",
      "GOOGL --- First date in year: 2020-07-04 19:46:18+00:00\n",
      "GOOGL --- First date in year: 2020-06-29 21:27:04+00:00\n",
      "GOOGL --- First date in year: 2020-06-25 15:58:54+00:00\n",
      "GOOGL --- First date in year: 2020-06-20 13:44:18+00:00\n",
      "GOOGL --- First date in year: 2020-06-15 18:14:03+00:00\n",
      "GOOGL --- First date in year: 2020-06-10 10:49:04+00:00\n",
      "GOOGL --- First date in year: 2020-06-05 01:52:10+00:00\n",
      "GOOGL --- First date in year: 2020-05-30 13:53:08+00:00\n",
      "GOOGL --- First date in year: 2020-05-25 12:35:50+00:00\n",
      "GOOGL --- First date in year: 2020-05-20 13:06:24+00:00\n",
      "GOOGL --- First date in year: 2020-05-15 10:00:01+00:00\n",
      "GOOGL --- First date in year: 2020-05-10 03:10:08+00:00\n",
      "GOOGL --- First date in year: 2020-05-05 06:45:23+00:00\n",
      "GOOGL --- First date in year: 2020-04-30 07:21:14+00:00\n",
      "GOOGL --- First date in year: 2020-04-25 02:00:24+00:00\n",
      "GOOGL --- First date in year: 2020-04-19 19:22:00+00:00\n",
      "GOOGL --- First date in year: 2020-04-14 18:50:26+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pracovn√≠\\AppData\\Local\\Temp\\ipykernel_13276\\2335487841.py:72: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2020-04-09 10:41:23+00:00\n",
      "GOOGL --- First date in year: 2020-04-03 11:48:24+00:00\n",
      "GOOGL --- First date in year: 2020-03-28 11:45:55+00:00\n",
      "GOOGL --- First date in year: 2020-03-22 07:12:47+00:00\n",
      "GOOGL --- First date in year: 2020-03-16 01:52:18+00:00\n",
      "GOOGL --- First date in year: 2020-03-09 08:00:00+00:00\n",
      "GOOGL --- First date in year: 2020-03-02 07:35:51+00:00\n",
      "GOOGL --- First date in year: 2020-02-24 14:09:14+00:00\n",
      "GOOGL --- First date in year: 2020-02-16 18:45:31+00:00\n",
      "GOOGL --- First date in year: 2020-02-08 13:16:42+00:00\n",
      "GOOGL --- First date in year: 2020-01-31 16:18:48+00:00\n",
      "GOOGL --- First date in year: 2020-01-23 09:30:00+00:00\n",
      "GOOGL --- First date in year: 2020-01-14 22:30:02+00:00\n",
      "GOOGL --- First date in year: 2020-01-06 22:11:08+00:00\n",
      "GOOGL --- First date in year: 2019-12-29 00:21:41+00:00\n",
      "GOOGL --- First date in year: 2019-12-20 00:38:55+00:00\n",
      "GOOGL --- First date in year: 2019-12-11 14:29:08+00:00\n",
      "GOOGL --- First date in year: 2019-12-03 14:18:18+00:00\n",
      "GOOGL --- First date in year: 2019-11-24 11:48:28+00:00\n",
      "GOOGL --- First date in year: 2019-11-15 08:02:34+00:00\n",
      "GOOGL --- First date in year: 2019-11-06 19:00:35+00:00\n",
      "GOOGL --- First date in year: 2019-10-28 20:11:02+00:00\n",
      "GOOGL --- First date in year: 2019-10-19 12:09:13+00:00\n",
      "GOOGL --- First date in year: 2019-10-10 02:22:20+00:00\n",
      "GOOGL --- First date in year: 2019-09-30 12:00:42+00:00\n",
      "updating-query, j:  0\n",
      "updating-query, j:  1\n",
      "GOOGL --- First date in year: 2019-09-20 19:12:11+00:00\n",
      "GOOGL --- First date in year: 2019-09-11 10:18:27+00:00\n",
      "updating-query, j:  0\n",
      "GOOGL --- First date in year: 2019-09-01 00:46:42+00:00\n",
      "GOOGL --- First date in year: 2019-08-22 13:09:45+00:00\n",
      "GOOGL --- First date in year: 2019-08-12 18:15:45+00:00\n",
      "GOOGL --- First date in year: 2019-08-02 12:38:43+00:00\n",
      "GOOGL --- First date in year: 2019-07-23 11:07:58+00:00\n",
      "GOOGL --- First date in year: 2019-07-12 12:34:35+00:00\n",
      "GOOGL --- First date in year: 2019-07-01 00:09:25+00:00\n",
      "GOOGL --- First date in year: 2019-06-20 14:02:22+00:00\n",
      "GOOGL --- First date in year: 2019-06-10 04:43:43+00:00\n",
      "GOOGL --- First date in year: 2019-05-31 17:01:46+00:00\n",
      "GOOGL --- First date in year: 2019-05-21 08:56:57+00:00\n",
      "GOOGL --- First date in year: 2019-05-10 20:22:32+00:00\n",
      "GOOGL --- First date in year: 2019-04-30 08:35:40+00:00\n",
      "GOOGL --- First date in year: 2019-04-19 03:00:25+00:00\n",
      "GOOGL --- First date in year: 2019-04-09 23:43:25+00:00\n",
      "GOOGL --- First date in year: 2019-03-30 11:04:28+00:00\n",
      "GOOGL --- First date in year: 2019-03-19 17:59:03+00:00\n",
      "GOOGL --- First date in year: 2019-03-08 10:55:18+00:00\n",
      "GOOGL --- First date in year: 2019-02-24 17:32:55+00:00\n",
      "GOOGL --- First date in year: 2019-02-13 03:35:25+00:00\n",
      "GOOGL --- First date in year: 2019-01-31 11:15:51+00:00\n",
      "GOOGL --- First date in year: 2019-01-19 13:22:39+00:00\n",
      "GOOGL --- First date in year: 2019-01-07 22:06:12+00:00\n",
      "GOOGL --- First date in year: 2018-12-26 14:06:41+00:00\n",
      "GOOGL --- First date in year: 2018-12-13 20:43:38+00:00\n",
      "GOOGL --- First date in year: 2018-12-02 08:47:03+00:00\n",
      "GOOGL --- First date in year: 2018-11-18 09:17:23+00:00\n",
      "GOOGL --- First date in year: 2018-11-04 11:01:52+00:00\n",
      "GOOGL --- First date in year: 2018-10-21 13:45:17+00:00\n",
      "GOOGL --- First date in year: 2018-10-09 16:48:37+00:00\n",
      "GOOGL --- First date in year: 2018-09-24 21:44:54+00:00\n",
      "GOOGL --- First date in year: 2018-09-10 16:02:56+00:00\n",
      "GOOGL --- First date in year: 2018-08-28 07:17:14+00:00\n",
      "GOOGL --- First date in year: 2018-08-15 14:05:00+00:00\n",
      "GOOGL --- First date in year: 2018-08-02 04:30:27+00:00\n",
      "GOOGL --- First date in year: 2018-07-18 17:43:01+00:00\n",
      "GOOGL --- First date in year: 2018-07-03 10:01:59+00:00\n",
      "GOOGL --- First date in year: 2018-06-17 14:04:51+00:00\n",
      "GOOGL --- First date in year: 2018-06-02 23:03:51+00:00\n",
      "GOOGL --- First date in year: 2018-05-19 01:28:10+00:00\n",
      "GOOGL --- First date in year: 2018-05-03 12:06:35+00:00\n",
      "GOOGL --- First date in year: 2018-04-16 15:19:52+00:00\n",
      "GOOGL --- First date in year: 2018-04-01 10:30:16+00:00\n",
      "GOOGL --- First date in year: 2018-03-16 16:25:41+00:00\n",
      "GOOGL --- First date in year: 2018-02-28 14:40:10+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not translate t.co card URL on tweet 966947686221049856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2018-02-11 03:13:30+00:00\n",
      "GOOGL --- First date in year: 2018-01-24 00:05:55+00:00\n",
      "GOOGL --- First date in year: 2018-01-07 10:36:22+00:00\n",
      "GOOGL --- First date in year: 2017-12-21 06:44:03+00:00\n",
      "GOOGL --- First date in year: 2017-12-03 02:44:55+00:00\n",
      "GOOGL --- First date in year: 2017-11-13 00:03:37+00:00\n",
      "GOOGL --- First date in year: 2017-10-20 14:05:21+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pracovn√≠\\AppData\\Local\\Temp\\ipykernel_13276\\2335487841.py:72: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2017-09-27 00:06:36+00:00\n",
      "GOOGL --- First date in year: 2017-09-01 23:06:19+00:00\n",
      "GOOGL --- First date in year: 2017-08-09 19:26:40+00:00\n",
      "GOOGL --- First date in year: 2017-07-17 15:02:43+00:00\n",
      "GOOGL --- First date in year: 2017-06-20 10:31:57+00:00\n",
      "GOOGL --- First date in year: 2017-05-21 01:30:31+00:00\n",
      "GOOGL --- First date in year: 2017-04-19 09:30:57+00:00\n",
      "GOOGL --- First date in year: 2017-03-20 16:16:02+00:00\n",
      "updating-query, j:  0\n",
      "updating-query, j:  0\n",
      "updating-query, j:  1\n",
      "updating-query, j:  0\n",
      "updating-query, j:  0\n",
      "updating-query, j:  1\n",
      "updating-query, j:  2\n",
      "GOOGL --- First date in year: 2017-02-20 04:05:40+00:00\n",
      "GOOGL --- First date in year: 2017-01-22 10:24:30+00:00\n",
      "GOOGL --- First date in year: 2016-12-22 01:06:05+00:00\n",
      "GOOGL --- First date in year: 2016-11-22 11:32:54+00:00\n",
      "GOOGL --- First date in year: 2016-10-18 23:04:24+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pracovn√≠\\AppData\\Local\\Temp\\ipykernel_13276\\2335487841.py:72: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2016-09-12 03:37:21+00:00\n",
      "GOOGL --- First date in year: 2016-08-05 23:06:37+00:00\n",
      "GOOGL --- First date in year: 2016-06-27 14:19:05+00:00\n",
      "updating-query, j:  0\n",
      "updating-query, j:  1\n",
      "GOOGL --- First date in year: 2016-05-14 10:59:46+00:00\n",
      "GOOGL --- First date in year: 2016-03-30 02:24:49+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not translate t.co card URL on tweet 712621441938632704\n",
      "Could not translate t.co card URL on tweet 708263560200060928\n",
      "Could not translate t.co card URL on tweet 702514095073792000\n",
      "Could not translate t.co card URL on tweet 699590700229226499\n",
      "Could not translate t.co card URL on tweet 698465482186182656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2016-02-11 20:13:15+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not translate t.co card URL on tweet 686555938682044418\n",
      "Could not translate t.co card URL on tweet 683401259848617984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2015-12-25 12:28:32+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pracovn√≠\\AppData\\Local\\Temp\\ipykernel_13276\\2335487841.py:72: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n",
      "Could not translate t.co card URL on tweet 667333141174992896\n",
      "Could not translate t.co card URL on tweet 666796903250591745\n",
      "Could not translate t.co card URL on tweet 666589836451561472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2015-10-29 09:49:03+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not translate t.co card URL on tweet 659014362166759424\n",
      "Could not translate t.co card URL on tweet 657906211245821952\n",
      "Could not translate t.co card URL on tweet 648971723900108800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGL --- First date in year: 2015-08-26 17:52:43+00:00\n",
      "GOOGL --- First date in year: 2015-06-17 20:26:06+00:00\n",
      "GOOGL --- First date in year: 2015-04-05 15:31:26+00:00\n",
      "GOOGL --- First date in year: 2015-01-18 01:27:24+00:00\n",
      "GOOGL --- First date in year: 2014-10-15 18:47:05+00:00\n",
      "GOOGL --- First date in year: 2014-06-29 05:18:18+00:00\n",
      "GOOGL --- First date in year: 2014-01-30 07:36:51+00:00\n",
      "GOOGL --- First date in year: 2013-01-26 00:26:36+00:00\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "since = '2012-01-01'\n",
    "until = '2022-05-01'\n",
    "\n",
    "since_unix =  str(int(time.mktime(datetime.datetime.strptime(since, \"%Y-%m-%d\").timetuple())))\n",
    "until_unix =  str(int(time.mktime(datetime.datetime.strptime(until, \"%Y-%m-%d\").timetuple())))\n",
    "\n",
    "\n",
    "ticker = \"GOOGL\" # SELECT TICKER!\n",
    "query = keywords[ticker] +' min_faves:100 since:{0} until:{1}'.format(since_unix,until_unix)\n",
    "last_query = query\n",
    "scraper = snscrape.modules.twitter.TwitterSearchScraper(query)\n",
    "i = 0\n",
    "j = 0\n",
    "stuck = False\n",
    "\n",
    "tweets_update = pd.DataFrame()\n",
    "gen = scraper.get_items()\n",
    "while True:\n",
    "    i+=1\n",
    "    try: \n",
    "        tweet = next(gen)\n",
    "        tweets_update = pd.concat([tweets_update, pd.DataFrame([{\"url\": tweet.url,\n",
    "                                        \"date\": tweet.date,\n",
    "                                        \"content\": tweet.content,\n",
    "                                        \"id\": tweet.id,\n",
    "                                        \"username\": tweet.user.username,\n",
    "                                        \"replyCount\": tweet.replyCount,\n",
    "                                        \"retweetCount\": tweet.retweetCount,\n",
    "                                        \"likeCount\": tweet.likeCount,\n",
    "                                        \"lang\": tweet.lang,\n",
    "                                        \"hashtags\": tweet.hashtags,\n",
    "                                        \"cashtags\": tweet.cashtags,\n",
    "                                        \"ticker\": ticker}])])\n",
    "        if not stuck:\n",
    "            j = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    except StopIteration:\n",
    "        tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n",
    "        tweets = pd.concat([tweets, tweets_update])\n",
    "        tweets.to_csv(\"data/tweets_keywords.csv\",index=False)\n",
    "        del tweets\n",
    "        gc.collect()\n",
    "        print('finished')\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "\n",
    "        \n",
    "        print(\"updating-query, j: \",j )\n",
    "        last_query = query\n",
    "        until_unix = str(int(time.mktime(datetime.datetime.strptime(str(tweets_update[\"date\"].min()), \"%Y-%m-%d %H:%M:%S%z\").timetuple()))+7000-j*3600)\n",
    "        query = keywords[ticker] +' min_faves:100 since:{0} until:{1}'.format(since_unix,until_unix)\n",
    "        scraper = snscrape.modules.twitter.TwitterSearchScraper(query)\n",
    "        gen = scraper.get_items()\n",
    "        j+=1\n",
    "        \n",
    "        if last_query==query:\n",
    "            stuck = True\n",
    "        else:\n",
    "            stuck = False\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print(ticker + ' --- First date in year: ' + str(pd.to_datetime(tweets_update.date,utc=True).min()))\n",
    "        tweets = pd.read_csv(\"data/tweets_keywords.csv\")\n",
    "        tweets = pd.concat([tweets, tweets_update])\n",
    "        tweets.to_csv(\"data/tweets_keywords.csv\",index=False)\n",
    "        del tweets\n",
    "        gc.collect()\n",
    "        tweets_update = pd.DataFrame()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
